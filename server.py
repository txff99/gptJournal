import time
from sentence_transformers import SentenceTransformer
import os
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), 'third_party', 'tidb-vector-python')))

from tidb_vector.integrations import TiDBVectorClient
from dotenv import load_dotenv


embed_model = SentenceTransformer("sentence-transformers/msmarco-MiniLM-L12-cos-v5", trust_remote_code=True)
embed_model_dims = embed_model.get_sentence_embedding_dimension()

def encode_chunks(chunks):
    embeddings = embed_model.encode(chunks)
    return embeddings.tolist()
load_dotenv()

# vector_store = TiDBVectorClient(
#    # The table which will store the vector data.
#    table_name='embedded_documents',
#    # The connection string to the TiDB cluster.
#    connection_string=os.environ.get('TIDB_DATABASE_URL'),
#    # The dimension of the vector generated by the embedding model.
#    vector_dimension=embed_model_dims,
#    # Determine whether to recreate the table if it already exists.
# #    drop_existing_table=True,
# )

# def print_result(query, result):
#    print(f"Search result (\"{query}\"):")
#    for r in result:
#       print(f"- text: \"{r.document}\", distance: {r.distance}")

query = "Say , Jim , how about going for a few beers after dinner?"
query_embedding = encode_chunks(query)
# now = time.time()
# search_result = vector_store.query(query_embedding, k=3)
# print(f"respond in {time.time()-now}")
# print_result(query, search_result)