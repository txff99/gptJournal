{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dialogue_file(file_path: str, numlines: int = None):\n",
    "    parsed_dialogues = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):    \n",
    "            if numlines is not None and i == numlines:  break\n",
    "            turns = line.strip().split('__eou__')\n",
    "            turns = [turn.strip() for turn in turns if turn.strip()]\n",
    "            parsed_dialogues.append(turns)\n",
    "\n",
    "    return parsed_dialogues\n",
    "\n",
    "def make_chunks(dialogues, chunk_size=8, padding=2):\n",
    "    all_sentences = [' ']*padding + [sentence for line in dialogues for sentence in line] + [' ']*padding\n",
    "    return [\n",
    "        ' '.join(all_sentences[i - padding:i + chunk_size + padding])\n",
    "        for i in range(padding, len(all_sentences)+padding, chunk_size)\n",
    "    ]\n",
    "\n",
    "file_path = 'dialogues_train.txt'\n",
    "parsed = parse_dialogue_file(file_path, numlines=10)\n",
    "chunks = make_chunks(parsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def encode_chunks(chunks):\n",
    "    model = SentenceTransformer('sentence-transformers/msmarco-MiniLM-L12-cos-v5', trust_remote_code=True)\n",
    "    embeddings = model.encode(chunks)\n",
    "    return embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.30492570287079035\n"
     ]
    }
   ],
   "source": [
    "# similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def encode_text(text, model):\n",
    "    return model.encode(text, convert_to_tensor=True)\n",
    "\n",
    "def compare_embeddings(embedding1, embedding2):\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])\n",
    "    return similarity[0][0]\n",
    "\n",
    "input = \"Jim, do you remeber the time i ask you out for beer?\"\n",
    "embed_out = encode_chunks(chunks    =chunks[0])\n",
    "embed_in = encode_chunks(input)\n",
    "\n",
    "similarity_score = compare_embeddings(embed_in, embed_out)\n",
    "print(f\"Similarity score: {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9914601339836674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(list1, list2):\n",
    "    # Convert lists to 2D arrays as required by scikit-learn\n",
    "    vec1 = [list1]\n",
    "    vec2 = [list2]\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "\n",
    "    # cosine_similarity returns a 2D array, so we need to extract the value\n",
    "    return similarity[0][0]\n",
    "list1 = [1, 2, 3]\n",
    "list2 = [1, 2, 4]\n",
    "similarity = calculate_cosine_similarity(list1, list2)\n",
    "print(f\"Cosine Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Say , Jim , how about going for a few beers after dinner ? You know that is tempting but is really not good for our fitness . What do you mean ? It will help us to relax . Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? I guess you are right.But what shall we do ? I don't feel like sitting at home . I suggest a walk over to the gym where we can play singsong and meet some of our friends . That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . Good.Let ' s go now . All right .\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0 Similarity score: 0.30492570287079035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 Similarity score: 0.22387367742001918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 2 Similarity score: 0.19060243650162173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 3 Similarity score: 0.1288242875911181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 4 Similarity score: -0.028113413475613434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 5 Similarity score: 0.05180320206651985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 6 Similarity score: 0.1178276812252784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 7 Similarity score: 0.16093966368950224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 8 Similarity score: 0.29293096181321504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 9 Similarity score: 0.20437515405198486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 10 Similarity score: 0.18013032452969915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 11 Similarity score: -0.15174401830351064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 12 Similarity score: -0.14995562429107617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawlet/miniconda3/envs/lchain/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 13 Similarity score: -0.14995562429107617\n"
     ]
    }
   ],
   "source": [
    "# compare with the one in the store\n",
    "chunks.append(\"What are your responsibilities in your present work ? My work involves various routine bookkeeping and basic accounting tasks including journal entries , verifying data and reconciling discrepancies , preparing detailed reports from raw data , and checking accounting documents for completeness , mathematical accuracy and consistency . Are you familiar with the PRC Financial and Tax Regulations ? I think so . Can you tell me something about this balance sheet now ? Of course . This balance sheet contains three major sections , that is , assets , liabilities and owner's equity . So , you see , the total current liabilities of your company are $ 3,372 , 000 , and the owner's equity is $ 5,400 , 000 . That means that the total assets , which is equal to the sum of the creditor's and the owner's equities , are $ 8,772 , 000 . What's the creditor's equity ? The creditor's equity is the same as liabilities . Will you tell me the situation ? I was in my friend's room talking for an hour of so . And then ? I came back to my room and found that my suitcase was open and my camera and five hundred dollars in cash inside the wallet were gone .\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    embed_out = encode_chunks(chunk)   \n",
    "    similarity_score = compare_embeddings(embed_in, embed_out)\n",
    "    print(f\"chunk {i} Similarity score: {similarity_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
